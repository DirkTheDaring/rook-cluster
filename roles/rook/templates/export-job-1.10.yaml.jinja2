apiVersion: batch/v1
kind: Job
metadata:
  name: export-vars-for-external
  namespace: {{ rook_namespace }} # namespace:cluster
  labels:
    app: export-vars-for-external
spec:
  template:
    spec:
      initContainers:
        - name: config-init
          image: rook/ceph:{{ helm_chart_version }}
          command: ["/usr/local/bin/toolbox.sh"]
          args: ["--skip-watch"]
          imagePullPolicy: IfNotPresent
          env:
            - name: ROOK_CEPH_USERNAME
              valueFrom:
                secretKeyRef:
                  name: rook-ceph-mon
                  key: ceph-username

          volumeMounts:
            - mountPath: /etc/ceph
              name: ceph-config
            - name: mon-endpoint-volume
              mountPath: /etc/rook
            - name: ceph-admin-secret
              mountPath: /var/lib/rook-ceph-mon
      containers:
        - name: script
          image: rook/ceph:{{ helm_chart_version }}
          volumeMounts:
            - mountPath: /etc/ceph
              name: ceph-config
              readOnly: true
          command:
            - "bash"
            - "-c"
            - |
              set -ex
              ceph status
              # FIXME This requires external access!!!
{% if rook_basic_version == "1.12" %}
              URL="https://raw.githubusercontent.com/rook/rook/release-1.11/deploy/examples/create-external-cluster-resources.py"
{% else %}
              URL="https://raw.githubusercontent.com/rook/rook/release-{{ rook_basic_version }}/deploy/examples/create-external-cluster-resources.py"
{% endif %}
              curl -o /tmp/create-external-cluster-resources.py -sL "$URL"
{% if helm_chart_version_canonical is version("1.13.0", '<' ) %}
              JSON=$(python3 /tmp/create-external-cluster-resources.py --rbd-data-pool-name ceph-filesystem-data0 --format json)
{% else %}
              JSON=$(python3 /tmp/create-external-cluster-resources.py --rbd-data-pool-name ceph-blockpool --format json)
              # if the the follwing line is required needs to be checked
              # ceph osd pool application disable ceph-filesystem-data0 rbd --yes-i-really-mean-it
{% endif %}
              ceph mgr module disable nfs
              # Source: https://docs.ceph.com/en/latest/mgr/orchestrator/#configuring-the-orchestrator-cli
              ceph mgr module enable rook
              ceph orch set backend rook

              MON_MAX_PG_PER_OSD_OLD=$(ceph config get mon mon_max_pg_per_osd)
              if [ $MON_MAX_PG_PER_OSD_OLD -gt 250 ]; then
                  MON_MAX_PG_PER_OSD_NEW=$MON_MAX_PG_PER_OSD_OLD
                  #if [ -n "rook_ceph_mon_max_pg_per_osd" ]; then
                  #fi
                  ceph config set mon mon_max_pg_per_osd $MON_MAX_PG_PER_OSD_NEW
                  MON_MAX_PG_PER_OSD_NEW=$(ceph config get mon mon_max_pg_per_osd)
                  echo "- mon_max_pg_per_osd: $MON_MAX_PG_PER_OSD_OLD"
                  echo "+ mon_max_pg_per_osd: $MON_MAX_PG_PER_OSD_NEW"
              fi
              set +x
              # All after ###autogen will be parsed as json
              echo "--- json output starts after this line"
              echo $JSON|jq

      volumes:
        - name: ceph-admin-secret
          secret:
            secretName: rook-ceph-mon
            optional: false
            items:
              - key: ceph-secret
                path: secret.keyring
        - name: mon-endpoint-volume
          configMap:
            name: rook-ceph-mon-endpoints
            items:
              - key: data
                path: mon-endpoints
        - name: ceph-config
          emptyDir: {}
      restartPolicy: Never
