#apiVersion: ceph.rook.io/v1
#kind: CephBlockPool
#metadata:
#  name:      replicapool
#  namespace: rook-ceph
#spec:
#  failureDomain: host
#  replicated:
#    size: 3
#    # Disallow setting pool with replica 1, this could lead to data loss without recovery.
#    # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
#    requireSafeReplicaSize: true
#    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
#    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
#    #targetSizeRatio: .5
#---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name:      {{ item.name }}
{% if 'annotations' in item %}
  annotations: 
    {{ item.annotations | to_yaml(indent=4) }}
{% endif  %}
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID:     rook-ceph
  pool:          "{{ item.parameters.pool if 'parameters' in item | default('replicapool') }}"
  imageFormat:   "2"
  imageFeatures: "layering"

  csi.storage.k8s.io/provisioner-secret-namespace:       {{ storage1_namespace }}
  csi.storage.k8s.io/provisioner-secret-name:            rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ storage1_namespace }}
  csi.storage.k8s.io/controller-expand-secret-name:      rook-csi-rbd-provisioner
  csi.storage.k8s.io/node-stage-secret-namespace:        {{ storage1_namespace }}
  csi.storage.k8s.io/node-stage-secret-name:             rook-csi-rbd-node

  csi.storage.k8s.io/fstype: ext4

allowVolumeExpansion: true
reclaimPolicy: {{ item.reclaimPolicy }}
